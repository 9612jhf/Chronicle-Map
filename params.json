{"name":"Chronicle-map","tagline":"Chronicle Map","body":"*We can help you get Chronicle up and running in your organisation, we suggest you invite us in for\r\nconsultancy, charged on an ad-hoc basis, we can discuss the best options tailored to your individual\r\nrequirements. - [Contact Us](http://openhft.net/support/)*\r\n\r\n*Or you may already be using Chronicle and just want some help - [find out more..](http://openhft.net/support/)*\r\n\r\n# Chronicle Map\r\n\r\nA low latency replicated Key Value Store across your network, with eventual consistency, persistence and performance.\r\n![Chronicle Map](http://openhft.net/wp-content/uploads/2014/07/ChronicleMap_200px.png)\r\n\r\n#### Maven Artifact Download\r\n```xml\r\n<dependency>                                   \r\n  <groupId>net.openhft</groupId>\r\n  <artifactId>chronicle-map</artifactId>\r\n  <version><!--replace with the latest version--></version>\r\n</dependency>\r\n```\r\nClick here to get the [Latest Version Number](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22net.openhft%22%20AND%20a%3A%22chronicle-map%22) \r\n\r\n#### Contents\r\n* [Overview](https://github.com/OpenHFT/Chronicle-Map#overview)\r\n* [Should I use Chronicle Queue or Chronicle Map](https://github.com/OpenHFT/Chronicle-Map#should-i-use-chroniclequeue-or-chroniclemap)\r\n* [What is the difference between SharedHashMap and Chronicle Map](https://github.com/OpenHFT/Chronicle-Map#what-is-the-difference-between-sharedhashmap-and-chroniclemap)\r\n* [JavaDoc](http://openhft.github.io/Chronicle-Map/apidocs)\r\n* [Getting Started Guide](https://github.com/OpenHFT/Chronicle-Map#getting-started)\r\n *  [Simple Construction](https://github.com/OpenHFT/Chronicle-Map#simple-construction)\r\n *   [Maven Download](https://github.com/OpenHFT/Chronicle-Map#maven-artifact-download-1)\r\n *   [Snapshot Download](https://github.com/OpenHFT/Chronicle-Map#maven-snapshot-download)\r\n *   [Key Value Object Types](https://github.com/OpenHFT/Chronicle-Map#key-value-object-types)\r\n *   [Off Heap and How to improve performance](https://github.com/OpenHFT/Chronicle-Map#off-heap-storage-and-how-using-a-proxy-object-can-improve-performance)\r\n *   [Sharing Data Between Two or More Maps](https://github.com/OpenHFT/Chronicle-Map#sharing-data-between-two-or-more-maps)\r\n *   [Entries](https://github.com/OpenHFT/Chronicle-Map#entries)\r\n *   [Size of Space Reserved on Disk](https://github.com/OpenHFT/Chronicle-Map#size-of-space-reserved-on-disk)\r\n *   [Chronicle Map Interface](https://github.com/OpenHFT/Chronicle-Map#chronicle-map-interface)\r\n* [Serialization](https://github.com/OpenHFT/Chronicle-Map#serialization)\r\n  *   [Simple Types](https://github.com/OpenHFT/Chronicle-Map#simple-types)\r\n  *   [Complex Types](https://github.com/OpenHFT/Chronicle-Map#complex-types)\r\n  *   [Import/Export entries as JSON](https://github.com/OpenHFT/Chronicle-Map#importexport-entries-as-json)\r\n* [Close](https://github.com/OpenHFT/Chronicle-Map#close)\r\n* [TCP / UDP Replication](https://github.com/OpenHFT/Chronicle-Map#tcp--udp-replication)\r\n * [TCP / UDP Background.](https://github.com/OpenHFT/Chronicle-Map#tcp--udp-background)\r\n *   [How to setup UDP Replication](https://github.com/OpenHFT/Chronicle-Map#how-to-setup-udp-replication)\r\n *  [TCP/IP Throttling](https://github.com/OpenHFT/Chronicle-Map#tcpip--throttling)\r\n *  [Replication How it works](https://github.com/OpenHFT/Chronicle-Map#replication-how-it-works)\r\n *  [Multiple Processes on the same server with Replication](https://github.com/OpenHFT/Chronicle-Map#multiple-processes-on-the-same-server-with-replication)\r\n *  [Identifier for Replication](https://github.com/OpenHFT/Chronicle-Map#identifier-for-replication)\r\n *  [Bootstrapping](https://github.com/OpenHFT/Chronicle-Map#bootstrapping)\r\n *    [Identifier](https://github.com/OpenHFT/Chronicle-Map#identifier)\r\n * [Port](https://github.com/OpenHFT/Chronicle-Map#port)\r\n * [Heart Beat Interval](https://github.com/OpenHFT/Chronicle-Map#heart-beat-interval)\r\n* [Multi Chronicle Maps - Network Distributed](https://github.com/OpenHFT/Chronicle-Map#multiple-chronicle-maps---network-distributed)\r\n* [Stateless Client](https://github.com/OpenHFT/Chronicle-Map#stateless-client)\r\n* [How to speed up the Chronicle Map Stateless Client](https://github.com/OpenHFT/Chronicle-Map#how-to-speed-up-the-chronicle-map-stateless-client)\r\n* [Questions/Answers](https://github.com/OpenHFT/Chronicle-Map#questions-and-answers)\r\n\r\n#### Miscellaneous\r\n\r\n * [Known Issues](https://github.com/OpenHFT/Chronicle-Map#known-issues)\r\n * [Stackoverflow](http://stackoverflow.com/tags/chronicle/info)\r\n * [Development Tasks - JIRA] (https://higherfrequencytrading.atlassian.net/browse/HCOLL)\r\n * [Use Case Which include Chronicle Map] (http://openhft.net/products/chronicle-engine/)\r\n \r\n#### Examples\r\n\r\n * [Hello World - A map which stores data off heap](https://github.com/OpenHFT/Chronicle-Map/blob/master/README.md#example--simple-hello-world)\r\n * [Sharing the map between two ( or more ) processes on the same computer](https://github.com/OpenHFT/Chronicle-Map/blob/master/README.md#example--sharing-the-map-on-two--or-more--processes-on-the-same-machine)\r\n * [Replicating data between process on different servers with TCP/IP Replication](https://github.com/OpenHFT/Chronicle-Map/blob/master/README.md#example--replicating-data-between-process-on-different-servers-via-tcp)\r\n * [Replicating data between process on different servers with UDP] (https://github.com/OpenHFT/Chronicle-Map/blob/master/README.md#example--replicating-data-between-process-on-different-servers-using-udp)\r\n *  [Creating a Chronicle Set and adding data to it](https://github.com/OpenHFT/Chronicle-Map/blob/master/README.md#example--creating-a-chronicle-set-and-adding-data-to-it)\r\n\r\n#### Performance Topics\r\n\r\n* [Chronicle Map with Large Data ](https://github.com/OpenHFT/Chronicle-Map#chronicle-map-with-large-data)\r\n* [Lock Contention] (https://github.com/OpenHFT/Chronicle-Map/blob/master/README.md#lock-contention)\r\n* [Better to use small keys](https://github.com/OpenHFT/Chronicle-Map#better-to-use-small-keys)\r\n* [ConcurrentHashMap v ChronicleMap](https://github.com/OpenHFT/Chronicle-Map#concurrenthashmap-v-chroniclemap)\r\n\r\n### Overview\r\nChronicle Map implements the interface `java.util.concurrent.ConcurrentMap`, however unlike the standard\r\njdk implementations, ChronicleMap is both persistent and able to share your entries accross processes:\r\n\r\n![](http://openhft.net/wp-content/uploads/2014/07/Chronicle-Map-diagram_04.jpg)\r\n\r\n## When to use HashMap, ConcurrentHashMap and ChronicleMap\r\n#### When to use HashMap\r\nIf you compare `HashMap`, `ConcurrentHashMap` and `ChronicleMap`, most of the maps in your system\r\nare likely to be HashMap.  This is because `HashMap` is lightweight.  Synchronized HashMap works\r\nwell for lightly contended use cases.  By contention we mean, how many threads on average are trying\r\nto use a Map.  One reason you can't have many contended resources, is that you only have so many\r\nCPUs and they can only be accessing a limited resources at once (ideally no more than one or two\r\nper thread at a time).\r\n\r\n####  When to use ConcurrentHashMap\r\n`ConcurrentHashMap` scales very well when highly contended.  It uses more memory but if you only\r\nhave a few of them, this doesn't matter.  They have higher throughput than the other two solutions,\r\nbut it does create the highest amount of garbage.  If garbage pressure is an issue for you (for example in \r\na low latency evironment), you should consider `ChronicleMap`.\r\n\r\n####  When to use ChronicleMap\r\nIf you have;\r\n* lots of small key-values\r\n* you want to minimise garbage produced, and medium lived objects.\r\n* you need to share data between JVMs\r\n* you need persistence\r\n\r\n#### Should I use ChronicleQueue or ChronicleMap\r\nChronicleQueue is 'lossless', designed to send every update. If your network can't do this something has\r\nto give. You could compress the data but at some point you have to work within the limits of your\r\nhardware or get more hardware. ChronicleMap on the other hand sends the latest value only.\r\nThis will naturally drop updates and is a more natural choice for low bandwidth connections.\r\n\r\n#### What is the difference between [SharedHashMap](https://github.com/OpenHFT/HugeCollections) and ChronicleMap\r\nSharedHashMap was the old name of what is now ChronicleMap. Since the last release of SharedHashMap\r\n we have added a lot of new features to ChronicleMap, most of these are listed in this readme.\r\n\r\n## Getting Started\r\n\r\n#### Tutorial 1 - Creating an instance of Chronicle Map\r\n[![ScreenShot](http://openhft.net/wp-content/uploads/2014/09/Screen-Shot-2014-10-14-at-17.49.36.png)](http://openhft.net/chronicle-map-video-tutorial-1/)\r\n\r\n### Simple Construction\r\n\r\nTo download the JAR which contains Chronicle Map, we recommend you use maven, which will download it\r\nfrom [Maven Central](http://search.maven.org), once you have installed maven, all you have to do is\r\nadd the following to your projects `pom.xml`:\r\n\r\n#### Maven Artifact Download\r\n```xml\r\n<dependency>\r\n  <groupId>net.openhft</groupId>\r\n  <artifactId>chronicle-map</artifactId>\r\n  <version><!--replace with the latest version--></version>\r\n</dependency>\r\n```\r\nTo get the latest version number\r\n[Click Here](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22net.openhft%22%20AND%20a%3A%22chronicle-map%22) \r\n\r\nwhen you add ( the above dependency ) to your pom maven will usually attempt to download the release artifacts from: \r\n```\r\nhttp://repo1.maven.org/maven2/net/openhft/chronicle-map\r\n```\r\n\r\n#### Maven Snapshot Download\r\nIf you want to try out the latest pre-release code, you can download the snapshot artifact manually from: \r\n```xml\r\nhttps://oss.sonatype.org/content/repositories/snapshots/net/openhft/chronicle-map/\r\n```\r\na better way is to add the following to your setting.xml, to allow maven to download snapshots :\r\n\r\n```xml\r\n<repository>\r\n    <id>Snapshot Repository</id>\r\n    <name>Snapshot Repository</name>\r\n    <url>https://oss.sonatype.org/content/repositories/snapshots</url>\r\n    <snapshots>\r\n        <enabled>true</enabled>\r\n    </snapshots>\r\n</repository>\r\n```\r\nand define the snapshot version in your pom.xml, for example:\r\n```xml\r\n<dependency>\r\n  <groupId>net.openhft</groupId>\r\n  <artifactId>chronicle-map</artifactId>\r\n  <version><!--replace with the latest version--></version>\r\n</dependency>\r\n```\r\n\r\n#### Key Value Object Types\r\n\r\nUnlike HashMap which will support any heap object, ChronicleMap only works with objects that it \r\ncan store off heap, so the objects have to be  :  (one of the following )\r\n\r\n- AutoBoxed primitives - for good performance.\r\n- Strings - for good performance.\r\n- implements Serializable  \r\n- implements Externalizable ( with a public default constructor ) \r\n- implements our custom interface BytesMarshallable ( with a public default constructor ) - use \r\nthis for best performance.\r\n\r\nor value objects that are created through, a directClass interface, for example : \r\n``` java\r\n      ChronicleMap<String, BondVOInterface> chm = ChronicleMapBuilder\r\n               .of(String.class, BondVOInterface.class)\r\n               .create();\r\n\r\n```\r\n\r\nObject graphs can also be included as long as the outer object supports Serializable, Externalizable or BytesMarshallable.\r\n\r\n#### ChronicleMap Construction\r\n\r\nCreating an instance of ChronicleMap is a little more complex than just calling a constructor.\r\nTo create an instance you have to use the ChronicleMapBuilder.\r\n\r\n``` java\r\nimport net.openhft.chronicle.map.*\r\n.....\r\n\r\ntry {\r\n\r\n    String tmp = System.getProperty(\"java.io.tmpdir\");\r\n    String pathname = tmp + \"/shm-test/myfile.dat\";\r\n\r\n    File file = new File(pathname);\r\n\r\n    ChronicleMapBuilder<Integer, CharSequence> builder =\r\n        ChronicleMapBuilder.of(Integer.class, CharSequence.class);\r\n    ConcurrentMap<Integer, CharSequence> map = builder.createPersistedTo(file);\r\n \r\n} catch (IOException e) {\r\n    e.printStackTrace();\r\n}\r\n```\r\n\r\nChronicleMap stores its data off the java heap, If you wish to share this off-heap memory between\r\nprocesses on the same server, you must provide a \"file\", this file must be the same \"file\" for all\r\nthe instances of Chronicle Map on the same server. The name and location of the \"file\" is entirely\r\nup to you.  For the best performance on many unix systems we recommend using\r\n[tmpfs](http://en.wikipedia.org/wiki/Tmpfs).\r\n\r\n### Sharing Data Between Two or More Maps\r\nSince this file is memory mapped, if you were to create another instance of the ChronicleMap,\r\npointing to the same file, both ChronicleMaps use this file as a common memory store, into which they\r\nboth read and write. The good thing about this, is that the two ( or more instances of the ChronicleMap )\r\ndon't have to be running in the same Java process. Ideally and for best performance, the two processes\r\nshould be running on the same server. Since the file is memory mapped, ( in most cases ) the read\r\nand writes to the file are hitting the disk cache. This allows the ChronicleMap to exchange data\r\nbetween processes by just using memory and in around 40 nanoseconds. \r\n\r\n``` java \r\nConcurrentMap<Integer, CharSequence> map1, map2;\r\n\r\n// this could be on one process\r\nmap1 = ChronicleMapBuilder.of(Integer.class, CharSequence.class).createPersistedTo(file);\r\n\r\n// this could be on the other process\r\nmap2 = ChronicleMapBuilder.of(Integer.class, CharSequence.class).createPersistedTo(file);\r\n```\r\nNote: In order to share data between map1 and map2, the file has to point to the same file location\r\non your server.\r\n\r\n### Entries\r\n\r\nOne of the differences with ChronicleMap against ConcurrentHashMap, is that it can't be resized.\r\nUnlike the ConcurrentHashMap, ChronicleMap is not limited to the available on heap memory.\r\nResizing is a very expensive operation for HashMaps as it can stall your application, this is why\r\nwe don't do it. When you are building a ChronicleMap you can set the maximum number of entries that\r\nyou are ever likely to support but it's fine to overestimate this number. This is because ChronicleMap is not\r\nlimited to your available memory therefore, at worst, you will end up having a large file on disk.\r\n\r\nYou set the maximum number of entries using the builder:\r\n\r\n``` java\r\nConcurrentMap<Integer, CharSequence> map =\r\n    ChronicleMapBuilder.of(Integer.class, CharSequence.class)\r\n    constantValueSizeBySample(\"a long sample string\"),\r\n    .entries(1000) // set the max number of entries here\r\n    .create();\r\n```\r\nIn this example above we have set 1000 entries.\r\n\r\n\r\nWe have optimised ChronicleMap so that you can have situations where you either don't use;\r\n\r\n- All the entries for which you have allowed.  This works best on Unix where the disk space and memory\r\nused reflect the number of actual entries, not the number for which you have allowed.\r\n\r\n- All the space you allow for each entry (see below).  This helps if you have entries which are multiple cache\r\nlines (128 bytes +). Only the lines you touch sit in your CPU cache and if you have multiple pages\r\n(8+ Kbytes) only the pages you touch use memory or disk.  The CPU cache usage matters as it can be\r\n10000x smaller than main memory.\r\n\r\n#### Entry size\r\n\r\nThe size of each entry depends on the type of the Keys and Values, as some types are larger than others. For example, if an entry uses Integers for both the keys and values, both the key and value each take exactly 4 bytes.\r\nThere is also some other overhead which is internal to chronicle, such as its internal multi-map, which has an overhead of\r\n* 6 bytes per entry for segment size of < 64k,\r\n* 9 bytes per entry for segment size of < 16m but > 64k\r\n* 12 bytes per entry for segment size of > 16m entries per segment.\r\n\r\nAlso, if you create a replicated map, there is an additional 10 bytes per entry.\r\n\r\nWe suggest you don't configure size for constant-sized keys or values, instead you can use the\r\nbuilder methods .constantKeySizeBySample(sampleKey) and\r\n.constantValueSizeBySample(sampleValue). For common types like Integer we suggest you don't use\r\nthese methods, for example ChronicleMap understands that Integer is 4 bytes long, Long is 8, etc.\r\n\r\n\r\n### Size of space reserved on disk\r\n\r\nIn Linux, if you looked at the size of the 'file', it will report the used entry size so if you\r\nhave just added one entry, it will report the size of this entry. Windows will report\r\nthe reserved size, as it reserves the disk space eagerly ( in fact windows also reserves the memory\r\neagerly as well ) in other words, number-of-entries x entry-size. \r\n\r\nso on Linux, if you type\r\n``` \r\n# It shows you the extents. \r\nls -l <file>\r\n\r\n# It shows you how much is actually used.\r\ndu <file>\r\n```\r\n\r\nTo illustrate this with an example - On Ubuntu we can create a 100 TB ChronicleMap.  Both `top` and\r\n`ls -l` say the process virtual size / file size is 100 TB, however the resident memory via `du`\r\nsays the size is 71 MB after adding 10000 entries. You can see the size actually used with du.\r\n\r\n### How Operating Systems differ\r\n\r\nAs a pure Java library, the same ChronicleMap Java byte code can be run on Windows, Linux and Mac OSX,\r\nhowever these operating systems work with memory mapped files differently. These differences effect how\r\nChronicleMap is able to map memory to a file and hence this can impact the total number of entries that you are\r\nable to configure.\r\n\r\n- Windows allocates memory and disk eagerly. Windows will fail if more than 4 GB is allocated in a single memory\r\nmapping, ( calculated as 4GB = 2^20 * 4 KB pages). Windows doesn't fail when a memory mapped region is mapped, rather it will fail when it is used up. This limitation doesn't apply to newer or server based versions of Windows. Eager memory allocation means you can't map more than free memory, but it should reduce jitter when you use it. In the future we may support multiple mappings to avoid this limitation, but there is no immediate plan to do so.\r\n- Linux allocates memory and disk lazily. Linux systems see a performance degradation at around 200% of main memory.\r\n- Mac OSX allocates memory lazily and disk eagerly.\r\n\r\nChronicleMap allocates head room which is a waste on Windows (Linux's sparse allocation means the head room has little\r\nimpact).\r\n\r\n##### For production\r\n- on Windows we recommend you use map sizes of less than 4 GB each, and less than 50% main memory in total.\r\n- on Linux we recommend you use small to large maps of less than double main memory. e.g. if you have a 128 GB server, we recommend you have less than 256 GB of maps on the server.\r\n- on Mac OSX, we have no specific recommendations.\r\n\r\n### ChronicleMap Interface\r\nThe ChronicleMap interface adds a few methods above and beyond the standard ConcurrentMap.\r\nThe ChronicleMapBuilder can also be used to return the ChronicleMap, see the example below :\r\n\r\n``` java\r\nChronicleMap<Integer, CharSequence> map =\r\n    ChronicleMapBuilder.of(Integer.class, CharSequence.class).create();\r\n```\r\nOne way to achieve good performance is to focus on unnecessary object creation, as this reduces\r\nthe amount of work that has to be carried out by the Garbage Collector. As such ChronicleMap\r\nsupports the following methods :\r\n\r\n - [`V getUsing(K key, V value);`](http://openhft.github.io/Chronicle-Map/apidocs/net/openhft/chronicle/map/ChronicleMap.html#getUsing-K-V-)\r\n - [`V acquireUsing(K key, V value);`](http://openhft.github.io/Chronicle-Map/apidocs/net/openhft/chronicle/map/ChronicleMap.html#acquireUsing-K-V-)\r\n - [`ReadContext<K, V> getUsingLocked(@NotNull K key, @NotNull V usingValue);`]  (https://github.com/OpenHFT/Chronicle-Map#off-heap-storage-and-how-using-a-proxy-object-can-improve-performance)\r\n - [`WriteContext<K, V> acquireUsingLocked(@NotNull K key, @NotNull V usingValue);`]    (https://github.com/OpenHFT/Chronicle-Map#acquireusinglocked)\r\n\r\nThese methods let you provide the object to which the data will be written so that the value used is mutable. For example\r\n\r\n``` java\r\nCharSequence using = new StringBuilder();\r\nCharSequence myResult = map.getUsing(\"key\", using);\r\n// at this point \"using\" and \"myResult\" will both point to the same object\r\n```\r\n\r\nThe `map.getUsing()` method is similar to `map.get()`, but because Chronicle Map stores its data off\r\nheap, if you were to call get(\"key\"), a new object would be created each time. map.getUsing() works\r\nby reusing the heap memory which was used by the original Object \"using\". This technique provides\r\nyou with better control over your object creation.\r\n\r\nExactly like `map.getUsing()`, `map.acquireUsing()` will give you back a reference to an value \r\nbased on\r\na key, but unlike `map.getUsing()` if there is no entry in the map for this key the entry \r\nwill be added and the value return will we the same value which you provided. ( The section below\r\n covers both `map.getUsing()` and `map.acquireUsing()` in more detail )\r\n\r\n#### Off heap storage and how using a Proxy Object can improve performance\r\n\r\nChronicleMap stores its data off heap. There are some distinct advantages in using off heap data storage\r\n\r\n * When the off heap data is backed by a memory mapped file, the entire map and its contents are automatically persisted. So if you have to restart your system, you won’t lose the content of your map.\r\n * Off heap data structures are not visited by the garbage collector.  With on heap maps like HashMap the garbage collector has to scan your entire object graph ( in fact just the live objects ) to remove garbage.  If you are able to keep your on heap footprint low, the garbage collector has a lot less work to do, this in turn will improve performance.\r\n * ChronicleMap and its data is serialised so it can be stored in off \r\nheap (shared) memory. When this memory is shared between processes on the same server, \r\nChronicleMap is able to distribute entries between processes with extremely low overhead. \r\nThis is because both processes are sharing the same off heap memory space. Additionaly, since your objects are already stored off heap ( as a series of bytes ), replicating entries over the network adds relatively little overhead.\r\n\r\nFor more information on the benefit's of off heap memory, see our article on - [On\r\nheap vs off\r\nheap memory\r\nusage]\r\n(http://vanillajava.blogspot.co.uk/2014/12/on-heap-vs-off-heap-memory-usage.html)\r\n\r\nOne of the downsides of an off heap map is that whenever you wish to get a value ( on heap ) from an entry\r\nwhich is off heap, for example calling :\r\n\r\n ``` java\r\nValue v = get(key)\r\n```\r\n\r\nthat entry has to be deserialised onto the Java heap so that you can use its value just like any other Java object. So if you were to call get(key) ten times, \r\n\r\n ``` java\r\nfor(int i=1;i<=10;i++) {\r\n  Value v = get(key)\r\n}\r\n``` \r\nthis would create 10 separate instances of the value. This is because each time get() is called, the map has to\r\nfirst create a Object to store the result in and then deserialise the value stored in the off \r\nheap entry. Assuming you want to get the value back on heap so that you can use it like a normal Java \r\nvalue, deserialisation is unavoidable.  Deserialisation has to occur every time, \r\n( since the value may have been changed by another thread ), but we don’t have to create the \r\nobject each time. This is why we introduced the method `getUsing(key,using)`. By reducing the number of objects\r\nyou create, the amount of work that the garbage collector has to carry out is reduced, \r\nthis in turn may improve overall performance. So back to `getUsing(key,using)`, if you wish to reuse\r\nand existing object ( in this case the ‘using’ value ), you can instead call :\r\n \r\n ``` java\r\nValue using = new Value();\r\n\r\nfor(int i=1;i<=10;i++) {\r\n  Value bond = map.getUsing(key,using); // this won’t create a new value each time.\r\n  assert using == bond; // this will always be the same instance and the Vaule type can not be immutable\r\n}\r\n``` \r\nWe can get a further performance improvement if we don't deserialize the whole object, \r\nbut deserialize only the bytes that in which we are actually interested, \r\n\r\nLets assume that we had the following interface :\r\n\r\n``` java\r\npublic interface LongValue {\r\n    long getValue();\r\n    void setValue(long value);\r\n}\r\n``` \r\nIt is possible to use chronicle as an off heap proxy that can go directly to the memory location off heap and just get back the long that you require. This is especially useful if the value object has a lot of fields in it like the BondVOInterface\r\n``` java\r\npublic interface BondVOInterface {\r\n ... \r\n\r\n    long getIssueDate();\r\n    void setIssueDate(long issueDate);  /* time in millis */\r\n\r\n    long getMaturityDate();\r\n    void setMaturityDate(long maturityDate);  /* time in millis */\r\n\r\n    double getCoupon();\r\n    void setCoupon(double coupon);\r\n\r\n    String getSymbol();\r\n    void setSymbol(@MaxSize(20) String symbol);\r\n ... \r\n}\r\n``` \r\nIf for example you want to just do the following\r\n\r\n``` java\r\nBondVOInterface  bond = map.get(key);\r\nbond.getCoupon()\r\n\r\n```\r\nlets say that it is only the `coupon` field that we are interested in, then its better not to have to\r\ndeserialise the whole object that implements the `BondVOInterface`. The `ChronicleMapBuilder` will look a the types of keys\r\n and values that you use, If the value type is a simple accessor/mutator interface that is exposing a non nested pojo, which uses simple types\r\n like `CharSequence` and primitives with corresponding get..() and\r\n set..() methods, Chronicle is able to generate off heap poxies so the whole object is not\r\n deserialized each\r\n  time it is accessed, The off heap proxies are able to read\r\nand write into\r\nthe off heap data structures directly, this reduced serialisation can give you a big performance boost.\r\nSee the example below which demonstartes how you can work directly with off heap entries.\r\n\r\n``` java\r\nChronicleMap<CharSequence, BondVOInterface> map = ChronicleMapBuilder\r\n        .of(String.class, BondVOInterface.class)\r\n        .keySize(10)\r\n        .create();\r\n```\r\nby default, builder assume that we are going to work directly with the off heap\r\nentries (`DataValueClasses.directClassFor(BondVOInterface.class)`).\r\n\r\nThe value class, in our case `BondVOInterface.class` is an `interface` rather than a `class`,  as before, we can\r\nuse the `getUsing(key,using)` method, but this time we have to create the ‘using’ instance slightly\r\ndifferently. We call either the `map.newValueInstance()` or `map.newKeyInstance()` method.\r\n\r\n``` java\r\nBondVOInterface using = map.newValueInstance();\r\n``` \r\n\r\nthe call to `getUsing(key,value)` is the same as we had in the earlier example\r\n\r\n``` java\r\nBondVOInterface using = map.newValueInstance();\r\nBondVOInterface  bond = map.getUsing(key,using);\r\nassert using == bond; // this will always be the same instance\r\n```\r\n\r\n`getUsing(key,using)` won’t create any on heap objects, and it won’t deserialize the ‘bond’ entry from off heap to on\r\nheap, all it does is sets the bond as a proxy to the off heap memory, this proxy object was created by\r\n`map.newValueInstance()`, it allows us access to the fields of our\r\nentry, directly into the off heap storage.\r\n\r\nAs above, ideally you would reuse the `using` variable.\r\n\r\n ``` java\r\nBondVOInterface using = map.newValueInstance();\r\n\r\nfor(int i=1;i<=10;i++) {\r\n  BondVOInterface bond = map.getUsing(key,using); // this won’t create a new value each time.\r\n  double coupon = bond.getCoupon()\r\n  assert using == bond; // this will always be the same instance\r\n}\r\n```\r\n\r\nso when you call :\r\n\r\n``` java\r\nbond.getCoupon()\r\n```\r\n\r\nNote: It is only the coupon that gets deserialized.\r\n\r\nJust like any other ConcurrentMap, ChronicleMap uses segment locking, if you wish to obtain a read lock\r\nwhen calling getUsing(key,using) you can do this :\r\n\r\n``` java\r\nBondVOInterface  bond = map.getUsing(key,value);\r\ntry (ReadContext<?, BondVOInterface> context = map.getUsingLocked(key,bond)) {\r\n   long issueDate =  bond.getIssueDate();\r\n   String symbol = bond.getSymbol();\r\n\r\n\t// add your logic here ( the lock will ensure this bond can not be changed by another thread )\r\n\r\n} // the lock is released here because close() is called\r\n\r\n```\r\n\r\nTo ensure that 'issueDate' and 'symbol' can be read atomically, these values\r\nmust be read while the segment read lock is in place.\r\n\r\nWhen you call map.getUsingLocked(key,using) we return a ReadContext, the ReadContext extends\r\nAutoCloseable so will automatically unlock the segment when the try block is exited.\r\n\r\nIf you wish not to use a try block you must manually release the segment lock by calling\r\n``` java\r\ncontext.close() //  releases the lock\r\n```\r\n\r\n####  acquireUsing()\r\nJust like getUsing(), acquireUsing() will also recycle the value you pass it. The following\r\ncode snippet is a pattern that you will often come across. `acquireUsing(key,value)` offers this \r\nfunctionality from a single method call, reducing the hash look-ups and making your code run slightly faster.\r\n\r\n ``` java\r\nV acquireUsing(key,value) {\r\n    Lock l = ...; // the segment lock of the map\r\n    l.lock();\r\n    try {\r\n           V v = map.getUsing(key,value)\r\n           if (v == null) {\r\n               map.put(key,value); // create a new entry\r\n               return value;\r\n           } else\r\n               return v; // return the value of the existing entry\r\n    } finally {\r\n        l.unlock();\r\n    }\r\n}\r\n```\r\n\r\n\r\n\r\n####  acquireUsingLocked()\r\n\r\nIf you are only accessing ChronicleMap from a single thread. If you are not doing replication\r\nand don't care about atomic reads. Then its simpler ( and faster ) to use `acquireUsing(key,\r\nvalue)` otherwise werecommend you use `acquireUsingLocked(key,value)` as this gives you atomicity.\r\n\r\nThe acquireUsingLocked(key,value) method holds a segment write lock, as it will update or put a\r\nnew entry into the map, this is unlike getUsingLocked\r\n(key,using) which only holds a segment read lock. Below is an example of how to use\r\nacquireUsingLocked(..,..).\r\n\r\n``` java\r\nBondVOInterface bond = ... // create your instance\r\ntry (WriteContext<?, BondVOInterface> context = map.acquireUsingLocked(\"one\", bond)) {\r\n assert bond ==  context.value();\r\n\r\n long issueDate =  bond.getIssueDate();\r\n String symbol = bond.getSymbol();\r\n\r\n if (condition)  // based on your own business logic\r\n    context.removeEntry();\r\n}\r\n```\r\n\r\n\r\nIf after some business logic, in our example after reading the 'issueDate' and\r\n'symbol', you wish to remove the entry, its more efficient to use the 'context' directly to remove the entry. The\r\n'context' is already aware of the entries location in memory. So it will be quicker to call\r\n`context.removeEntry()` rather than `map.remove(key)`.\r\n\r\n## Serialization\r\n\r\n![Serialization](http://openhft.net/wp-content/uploads/2014/09/Serialization_01.jpg)\r\n\r\n\r\nChronicleMap stores your data into off heap memory, so when you give it a Key or Value, it will\r\nserialise these objects into bytes.\r\n\r\n### Simple Types\r\nIf you are using simple auto boxed objects based on the primitive types, ChronicleMap will\r\nautomatically handle the serialisation for you.  \r\n\r\n### Complex Types\r\nFor anything other than the standard object, the Objects either have to :\r\n* implement \"java.io.Serializable\" ( which we don't recommend as this can be slow )\r\n* we also support \"java.io.Externalizable\", we recommend this over Serializable as its usually faster.\r\n* or for the best performance implement net.openhft.lang.io.serialization.BytesMarshallable,\r\nan example of how to do this can be found at \"IntValue$$Native\"\r\n* alternatively, you could write a \"Custom Marshaller\", the custom marshaller can be implemented\r\nfor a single type or a number of types.\r\n\r\n### Import/Export entries as JSON\r\n![Import/Export](http://openhft.net/wp-content/uploads/2014/09/Export-import_04.jpg)\r\n\r\nChronicleMap supports importing and exporting all the entries into a JSON encoded file.\r\n\r\n``` java\r\nvoid getAll(File toFile) throws IOException;\r\nvoid putAll(File fromFile) throws IOException;\r\n```\r\n\r\nOnly the entries of your map are exported, not the configuration of your map. So care\r\nmust be taken to populate the data in to a map of the correct Key/Value type and with enough\r\navailable entries. When importing data :\r\n\r\n* entries that are in the map but not in the JSON file will remain untouched.\r\n* entries that are in the map and in the JSON file will be updated.\r\n* entries that are not in the map but are in the JSON file will be added.\r\n\r\nIn other words importing data into a ChronicleMap works like `map.putAll(<JSON entries>)`.\r\n\r\nWhen importing data, if you are also writing to the map at the same time, the last update will win.\r\nIn other words a write lock is not held for the entire import process.\r\nImporting and exporting the map, is ideal if you wish to:\r\n* Bulk load data from one ChronicleMap into another.\r\n* migrate data between versions of ChronicleMap.\r\n\r\n\r\n## Close\r\nUnlike ConcurrentHashMap, ChronicleMap stores its data off heap, often in a memory mapped file.\r\nIts recommended that you call close() once you have finished working with a ChronicleMap.\r\n\r\n``` java\r\nmap.close()\r\n```\r\n\r\nThis is especially important when working with ChronicleMap replication, as failure to call close may prevent\r\nyou from restarting a replicated map on the same port. In the event that your application crashes it may not\r\nbe possible to call close(). Your operating system will usually close dangling ports automatically,\r\nso although it is recommended that you close() when you have finished with the map,\r\nits not something that you must do, it's just something that we recommend you should do.\r\n\r\n###### WARNING\r\n\r\nIf you call close() too early before you have finished working with the map, this can cause\r\nyour JVM to crash. Close MUST BE the last thing that you do with the map.\r\n\r\n\r\n# TCP / UDP Replication\r\n\r\nChronicleMap supports both TCP and UDP replication\r\n\r\n![TCP/IP Replication](http://openhft.net/wp-content/uploads/2014/07/Chronicle-Map-TCP-Replication_simple_02.jpg)\r\n\r\n### TCP / UDP Background.\r\nTCP/IP is a reliable protocol, what this means is that unless you have a network failure or hardware\r\noutage the data is guaranteed to arrive. TCP/IP provides point to point connectivity. So in effect\r\n( over simplified ), if the message was sent to 100 hosts, the message would have to be sent\r\n100 times. With UDP, the message is only sent once. This is ideal if you have a large number of\r\nhosts and you wish to broadcast the same data to each of them.   However, one of the big drawbacks\r\nwith UDP is that it's not a reliable protocol. This means, if the UDP message is Broadcast onto\r\nthe network, the hosts are not guaranteed to receive it, so they can miss data. Some solutions\r\nattempt to build resilience into UDP, but arguably, this is in effect reinventing TCP/IP.\r\n\r\n### How to setup UDP Replication\r\nIn reality on a good quality wired LAN, when using UDP, you will rarely miss messages. Nevertheless this is\r\na risk that we suggest you don't take. We suggest that whenever you use UDP replication you use it\r\nin conjunction with a throttled TCP replication, therefore if a host misses a message over UDP, they\r\nwill later pick it up via TCP/IP. \r\n\r\n###  TCP/IP  Throttling\r\nWe are careful not to swamp your network with too much TCP/IP traffic, We do this by providing\r\na throttled version of TCP replication. This works because ChronicleMap only broadcasts the latest\r\nupdate of each entry. \r\n\r\n### Replication How it works\r\n\r\nChronicleMap provides multi master hash map replication. What this means, is that each remote\r\nmap, mirrors its changes over to another remote map, neither map is considered\r\nthe master store of data. Each map uses timestamps to reconcile changes.\r\nWe refer to in instance of a remote map as a node.\r\nA node can be connected to up to 128 other nodes.\r\nThe data that is stored locally in each node becomes eventually consistent. So changes made to one\r\nnode, for example by calling put(), will be replicated over to the other node. To achieve a high\r\nlevel of performance and throughput, the call to put() won’t block, \r\nWith ConcurrentHashMap, It is typical to check the return code of some methods to obtain the old\r\nvalue, for example remove(). Due to the loose coupling and lock free nature of this multi master\r\nimplementation,  this return value is only the old value on the nodes local data store. In other\r\nwords the nodes are only concurrent locally. Its worth realising that another node performing\r\nexactly the same operation may return a different value. However reconciliation will ensure the maps\r\nthemselves become eventually consistent.\r\n\r\n### Reconciliation \r\nIf two ( or more nodes ) receive a change to their maps for the same key but different values, say\r\nby a user of the maps, calling the put(key,value), then, initially each node will update its local\r\nstore and each local store will hold a different value. The aim of multi master replication is\r\nto provide eventual consistency across the nodes. So, with multi master whenever a node is changed\r\nit will notify the other nodes of its change. We will refer to this notification as an event.\r\nThe event will hold a timestamp indicating the time the change occurred, it will also hold the state\r\ntransition, in this case it was a put with a key and value.\r\nEventual consistency is achieved by looking at the timestamp from the remote node, if for a given\r\nkey, the remote nodes timestamp is newer than the local nodes timestamp, then the event from\r\nthe remote node will be applied to the local node, otherwise the event will be ignored. Since\r\nnone of the nodes is a primary, each node holds information about the other nodes. For this node its\r\nown identifier is referred to as its 'localIdentifier', the identifiers of other nodes are the\r\n'remoteIdentifiers'. On an update or insert of a key/value, this node pushes the information of\r\nthe change to the remote nodes. The nodes use non-blocking java NIO I/O and all replication is done\r\non a single thread. However there is an edge case. If two nodes update their map at the same time\r\nwith different values, we have to deterministically resolve which update wins. This is because eventual\r\nconsistency mandates that both nodes should end up locally holding the same data. Although it is rare that two remote\r\nnodes receive an update to their maps at exactly the same time for the same key, we have\r\nto handle this edge case.  We can not therefore rely on timestamps alone to reconcile\r\nthe updates. Typically the update with the newest timestamp should win, but in this example both\r\ntimestamps are the same, and the decision made to one node should be identical to the decision made\r\nto the other. This dilemma is resolved by using a node identifier, the node identifier is a unique\r\n'byte' value that is assigned to each node. When the time stamps are the same the remote node with the\r\nsmaller identifier will be preferred.\r\n\r\n### Multiple Processes on the same server with Replication\r\n\r\nOn a server if you have a number of Java processes and then within each Java process you create an instance of a ChronicleMap which binds to the same underline 'file', they exchange data via shared memory rather than TCP or UDP replication. So if a ChronicleMap which is not performing TCP Replication is updated, this update can be picked up by another ChronicleMap. This other ChronicleMap could be a TCP replicated ChronicleMap. In such an example the TCP replicated ChronicleMap would then push the update to the remote nodes.\r\n\r\nLikewise, if the TCP replicated ChronicleMap was to received an update from a remote node, then this update would be immediately available to all the ChronicleMaps on the server.\r\n\r\n### Identifier for Replication\r\nIf all you are doing is replicating your ChronicleMaps on the same server you don't have to set up\r\nTCP and UDP replication. You also don't have to set the identifiers - as explained earlier this identifier is only for the resolution of conflicts amongst remote servers.\r\n\r\nIf however you wish to replicate data between 2 or more servers, then ALL of the ChronicleMaps\r\nincluding those not actively participating in TCP or UDP replication must have the identifier set.\r\nThe identifier must be unique to each server. Each ChronicleMap on the same server must have\r\nthe same identifier. The reason that all ChronicleMaps must have the identifier set, is because\r\nthe memory is laid out slightly differently when using replication, so even if a Map is not actively\r\nperforming TCP or UDP replication itself, if it wishes to replicate with one that is, it must have\r\nits memory laid out the same way to be compatible. \r\n\r\nIf the identifiers are not set up uniquely then the updates will be ignored, as for example\r\na ChronicleMap set up with the identifiers equals '1', will ignore all events which contain\r\nthe remote identifier of '1', in other words Chronicle Map replication is set up to ignore updates\r\nwhich have originated from itself. This is to avoid the circularity of events.\r\n\r\nWhen setting up the identifier you can use values from 1 to 127. ( see the section above for more\r\ninformation on identifiers and how they are used in replication. )\r\n\r\nThe identifier is setup on the builder as follows.\r\n\r\n```java\r\nTcpTransportAndNetworkConfig tcpConfig = ...\r\nmap = ChronicleMapBuilder\r\n    .of(Integer.class, CharSequence.class)\r\n    .replication(identifier, tcpConfig)\r\n    .create();\r\n```\r\n\r\n\r\n### Bootstrapping \r\nWhen a node is connected over the network to an active grid of nodes. It must first receive any data\r\nthat it does not have from the other nodes. Eventually, all the nodes in the grid have to hold a\r\ncopy of exactly the same data. We refer to this initial data load phase as bootstrapping.\r\nBootstrapping by its very nature is point to point, so it is only performed over TCP replication.\r\nFor architectures that wish to use UDP replication it is advised you use TCP Replication as well. A\r\ngrid which only uses UDP replication will miss out on the bootstrapping, possibly leaving the nodes\r\nin an inconsistent state. To avoid this, if you would rather reduce the amount of TCP traffic on\r\nyour network, we suggest you consider using a throttle TCP replication along with UDP replication.\r\nBootstrapping is not used when the nodes are on the same server, so for this case, TCP replication\r\nis not required.\r\n\r\n### Identifier\r\nEach map is allocated a unique identifier\r\n\r\nServer 1 has:\r\n```\r\n.replication((byte) 1, tcpConfigServer1)\r\n```\r\n\r\nServer 2 has:\r\n```\r\n.replication((byte) 2, tcpConfigServer2)\r\n```\r\n\r\nServer 3 has:\r\n```\r\n.replication((byte) 3, tcpConfigServer3)\r\n```\r\n\r\nIf you fail to allocate a unique identifier replication will not work correctly.\r\n\r\n### Port\r\nEach map must be allocated a unique port, the port has to be unique per server, if the maps are\r\nrunning on different hosts they could be allocated the same port, but in our example we allocated\r\nthem different ports, we allocated map1 port 8076 and map2 port 8077. Currently we don't support\r\ndata forwarding, so it important to connect every remote map, to every other remote map, in other\r\nwords you can't have a hub configuration where all the data passes through a single map which every\r\nother map is connected to. So currently, if you had 4 servers each with a Chronicle Map, you would\r\nrequire 6 connections.\r\n\r\nIn our case we are only using 2 maps, this is how we connected map1 to map 2.\r\n```\r\nTcpTransportAndNetworkConfig.of(8076, new InetSocketAddress(\"localhost\", 8077))\r\n                    .heartBeatInterval(1, SECONDS);\r\n```\r\nyou could have put this instruction on map2 instead, like this \r\n```\r\nTcpTransportAndNetworkConfig.of(8077, new InetSocketAddress(\"localhost\", 8076))\r\n                    .heartBeatInterval(1, SECONDS);\r\n```\r\neven though data flows from map1 to map2 and map2 to map1 it doesn't matter which way you connected\r\nthis, in other words its a bidirectional connection. \r\n\r\n### Configuring Three Way TCP/IP Replication\r\n\r\n![TCP/IP Replication 3Way](http://openhft.net/wp-content/uploads/2014/09/Screen-Shot-2014-10-27-at-18.19.05.png)\r\n\r\nBelow is example how to set up tcpConfig for 3 host\r\n\r\n```java\r\nString hostServer1 = \"localhost\"; // change this to your host\r\nint serverPort1 = 8076;           // change this to your port\r\nInetSocketAddress inetSocketAddress1 = new InetSocketAddress(hostServer1, serverPort1);\r\n\r\nString hostServer2 = \"localhost\"; // change this to your host\r\nint  serverPort2= 8077;           // change this to your port\r\nInetSocketAddress inetSocketAddress2 = new InetSocketAddress(hostServer2, serverPort2);\r\n\r\nString hostServer3 = \"localhost\"; // change this to your host\r\nint serverPort3 = 8078;           // change this to your port\r\nInetSocketAddress inetSocketAddress3 = new InetSocketAddress(hostServer3, serverPort3);\r\n\r\n// this is to go on server 1\r\nTcpTransportAndNetworkConfig tcpConfigServer1 =\r\n        TcpTransportAndNetworkConfig.of(serverPort1);\r\n\r\n// this is to go on server 2\r\nTcpTransportAndNetworkConfig tcpConfigServer2 = TcpTransportAndNetworkConfig\r\n        .of(serverPort2, inetSocketAddress1);\r\n\r\n// this is to go on server 3\r\nTcpTransportAndNetworkConfig tcpConfigServer3 = TcpTransportAndNetworkConfig\r\n        .of(serverPort3, inetSocketAddress1, inetSocketAddress2);\r\n```     \r\n\r\n### Heart Beat Interval\r\nWe set a heartBeatInterval, in our example to 1 second\r\n``` java\r\n heartBeatInterval(1, SECONDS)\r\n```\r\nA heartbeat will only be send if no data is transmitted, if the maps are constantly exchanging data\r\nno heartbeat message is sent. If a map does not receive either data of a heartbeat the connection\r\nis dropped and re-established.\r\n\r\n# Multiple Chronicle Maps - Network Distributed\r\n\r\n![Chronicle Maps Network Distributed](http://openhft.net/wp-content/uploads/2014/07/Chronicle-Map_channels_diagram_02.jpg)\r\n\r\nChronicleMap TCP Replication lets you distribute a single ChronicleMap, to a number of servers\r\nacross your network. Replication is point to point and the data transfer is bidirectional, so in the\r\nexample of just two servers, they only have to be connected via a single TCP socket connection and\r\nthe data is transferred both ways. This is great, but what if you wanted to replicate more than\r\njust one ChronicleMap, what if you were going to replicate two ChronicleMaps across your network,\r\nunfortunately with just TCP replication you would have to have two tcp socket connections, which is\r\nnot ideal. This is why we created the `ReplicationHub`. The `ReplicationHub` lets you replicate numerous\r\nChronicleMaps via a single point to point socket connection.\r\n\r\nThe `ReplicationHub` encompasses TCP replication, where each map has to be given a\r\nunique identifier, but when using the `ReplicationHub` we use a channel to identify the map,\r\nrather than the identifier.  The identifier is used to identify the host/server which broadcasts the\r\nupdate. Put simply:\r\n\r\n* Each host must be given a unique identifier.\r\n* Each map must be given a unique channel.\r\n\r\n``` java\r\nbyte identifier= 2;\r\nReplicationHub replicationHub = ReplicationHub.builder()\r\n                    .tcpTransportAndNetwork(tcpConfig)\r\n                    .createWithId(identifier);\r\n```\r\n\r\nIn this example above the `ReplicationHub` is given the identifier of 2.\r\n\r\nWith channels you are able to attach additional maps to a `ReplicationChannel` once its up and\r\nrunning.\r\n\r\nWhen creating the `ReplicationChannel` you should attach your tcp or udp configuration :\r\n``` java\r\nbyte identifier = 1;\r\nReplicationHub replicationHub = ReplicationHub.builder()\r\n                    .tcpTransportAndNetwork(tcpConfig)\r\n                    .createWithId(identifier);\r\n```\r\n\r\nAttaching a `ReplicationChannel` to the map :\r\n\r\n``` java\r\nshort channel = (short) 2;\r\nChronicleMap<Integer, CharSequence> map = ChronicleMapBuilder.of(Integer.class, CharSequence.class)\r\n  .entries(1000)\r\n  .instance().replicatedViaChannel(replicationHub.createChannel(channel))\r\n  .create();\r\n```\r\n\r\nThe Chronicle channel is use to identify which map is to be replicated to which other map on\r\nthe remote node. In the example above this is assigned to '(short) 1', so for example if you have\r\ntwo maps, lets call them map1 and map2, you could assign them with chronicle\r\nchannels 1 and 2 respectively. Map1 would have the chronicle channel of 1 on both servers. You\r\nshould not confuse the Chronicle Channels with the identifiers, the identifiers are unique per\r\nreplicating node ( in this case which host, the reason we say replicating node rather than host as it is\r\npossible to have more than one replicating node per host if each of them had a different TCP/IP port ), where as the chronicle channels are used to identify which map you are referring. No additional socket\r\n connection is made per chronicle channel that\r\nyou use, so we allow up to 32767 chronicle channels.\r\n\r\nIf you inadvertently got the chronicle channels around the wrong way, then chronicle would attempt\r\nto replicate the wrong maps data. The chronicle channels don't have to be in order but they must be\r\nunique for each map you have.\r\n\r\n### Channels and ReplicationChannel - Example\r\n\r\n``` java\r\n\r\nimport net.openhft.chronicle.hash.replication.ReplicationChannel;\r\nimport net.openhft.chronicle.hash.replication.ReplicationHub;\r\nimport net.openhft.chronicle.hash.replication.TcpTransportAndNetworkConfig;\r\nimport java.net.InetSocketAddress;\r\nimport java.util.concurrent.TimeUnit;\r\n\r\nimport static org.junit.Assert.assertEquals;\r\n\r\n...\r\n\r\nChronicleMap<CharSequence, CharSequence> favoriteColourServer1, favoriteColourServer2;\r\nChronicleMap<CharSequence, CharSequence> favoriteComputerServer1, favoriteComputerServer2;\r\n\r\n\r\n// server 1 with  identifier = 1\r\n{\r\n    ChronicleMapBuilder<CharSequence, CharSequence> builder =\r\n            ChronicleMapBuilder.of(CharSequence.class, CharSequence.class).entries(1000);\r\n\r\n    byte identifier = (byte) 1;\r\n\r\n    TcpTransportAndNetworkConfig tcpConfig = TcpTransportAndNetworkConfig\r\n            .of(8086, new InetSocketAddress(\"localhost\", 8087))\r\n            .heartBeatInterval(1, TimeUnit.SECONDS);\r\n\r\n    ReplicationHub hubOnServer1 = ReplicationHub.builder()\r\n            .tcpTransportAndNetwork(tcpConfig)\r\n            .createWithId(identifier);\r\n\r\n    // this demotes favoriteColour\r\n    short channel1 = (short) 1;\r\n\r\n    ReplicationChannel channel = hubOnServer1.createChannel(channel1);\r\n    favoriteColourServer1 = builder.instance()\r\n            .replicatedViaChannel(channel).create();\r\n\r\n    favoriteColourServer1.put(\"peter\", \"green\");\r\n\r\n    // this demotes favoriteComputer\r\n    short channel2 = (short) 2;\r\n\r\n    favoriteComputerServer1 = builder.instance()\r\n            .replicatedViaChannel(hubOnServer1.createChannel(channel2)).create();\r\n\r\n    favoriteComputerServer1.put(\"peter\", \"dell\");\r\n}\r\n\r\n// server 2 with  identifier = 2\r\n{\r\n    ChronicleMapBuilder<CharSequence, CharSequence> builder =\r\n            ChronicleMapBuilder.of(CharSequence.class, CharSequence.class).entries(1000);\r\n\r\n    byte identifier = (byte) 2;\r\n\r\n    TcpTransportAndNetworkConfig tcpConfig = TcpTransportAndNetworkConfig\r\n            .of(8087).heartBeatInterval(1, TimeUnit.SECONDS);\r\n\r\n    ReplicationHub hubOnServer2 = ReplicationHub.builder()\r\n            .tcpTransportAndNetwork(tcpConfig)\r\n            .createWithId(identifier);\r\n\r\n    // this demotes favoriteColour\r\n    short channel1 = (short) 1;\r\n\r\n    favoriteColourServer2 = builder.instance()\r\n            .replicatedViaChannel(hubOnServer2.createChannel(channel1)).create();\r\n\r\n    favoriteColourServer2.put(\"rob\", \"blue\");\r\n\r\n    // this demotes favoriteComputer\r\n    short channel2 = (short) 2;\r\n\r\n    favoriteComputerServer2 = builder.instance()\r\n            .replicatedViaChannel(hubOnServer2.createChannel(channel2)).create();\r\n\r\n    favoriteComputerServer2.put(\"rob\", \"mac\");\r\n    favoriteComputerServer2.put(\"daniel\", \"mac\");\r\n}\r\n\r\n// allow time for the recompilation to resolve\r\nfor (int t = 0; t < 2500; t++) {\r\n    if (favoriteComputerServer2.equals(favoriteComputerServer1) &&\r\n            favoriteColourServer2.equals(favoriteColourServer1))\r\n        break;\r\n    Thread.sleep(1);\r\n}\r\n\r\nassertEquals(favoriteComputerServer1, favoriteComputerServer2);\r\nassertEquals(3, favoriteComputerServer2.size());\r\n\r\nassertEquals(favoriteColourServer1, favoriteColourServer2);\r\nassertEquals(2, favoriteColourServer1.size());\r\n\r\nfavoriteColourServer1.close();\r\nfavoriteComputerServer2.close();\r\nfavoriteColourServer2.close();\r\nfavoriteColourServer1.close();\r\n\r\n``` \r\n\r\n# Stateless Client\r\n\r\n![](http://openhft.net/wp-content/uploads/2014/07/Chronicle-Map-remote-stateless-map_04_vB.jpg)\r\n\r\nA stateless client is an instance of a `ChronicleMap` or a `ChronicleSet` that does not hold any \r\ndata\r\n locally, all the Map or Set operations are delegated via a Remote Procedure Calls ( RPC ) to \r\n another `ChronicleMap` or  `ChronicleSet`  which we will refer to as the server. The server\r\n holds all your data, the server can not it’s self be a stateless client. Your stateless client must\r\n be connected to the server via TCP/IP.\r\n\r\n ![ChronicleMap](http://openhft.net/wp-content/uploads/2014/09/State-Transition_1-thread_02.jpg)\r\n\r\n The stateless client delegates all your method calls to\r\n the remote server. The stateless client operations will block, in other words the stateless\r\n client waits for the server to send a response before continuing to the next operation. The stateless\r\n client could be  consider to be a ClientProxy to `ChronicleMap` or  `ChronicleSet`  running\r\n on another host.\r\n \r\n Below is an example of how to configure a stateless client.\r\n\r\n``` java\r\nfinal ChronicleMap<Integer, CharSequence> serverMap;\r\nfinal ChronicleMap<Integer, CharSequence> statelessMap;\r\n\r\n// server\r\n{\r\n\r\n    ChronicleMapBuilder.of(Integer.class, CharSequence.class)\r\n            .replication((byte) 2, TcpTransportAndNetworkConfig.of(8076))\r\n            .create();            \r\n                       \r\n    serverMap.put(10, \"EXAMPLE-10\");\r\n}\r\n\r\n// stateless client\r\n{\r\n    statelessMap = ChronicleMapBuilder.of(Integer.class, CharSequence.class)\r\n            .statelessClient(new InetSocketAddress(\"localhost\", 8076))\r\n            .create();\r\n\r\n    Assert.assertEquals(\"EXAMPLE-10\", statelessMap.get(10));\r\n    Assert.assertEquals(1, statelessMap.size());\r\n}\r\n\r\nserverMap.close();\r\nstatelessMap.close();\r\n```\r\n\r\nWhen used with a stateless client, each statefull server has to be configured with TCP \r\nreplication, when you set up TCP replication you must define a port for the replication to \r\nrun on, the port you choose is up to you, but you should pick a free port that is not currently \r\nbeing used by another application. In this example we choose the port 8076\r\n\r\n``` java\r\n// sets the server to run on localhost:8076\r\n.replication((byte) 2, TcpTransportAndNetworkConfig.of(8076))\r\n``` \r\n  \r\nOn the \"stateless client\" we connect to the server via TCP/IP on localhost:8076 : \r\n\r\n``` java\r\n.statelessClient(new InetSocketAddress(\"localhost\", 8076))\r\n```\r\n\r\nbut in your example you should choose the host of the statefull server and the port you allocated\r\n it. \r\n\r\n``` java\r\n.statelessClient(new InetSocketAddress(<host of state-full server>, <port of state-full server>))\r\n```\r\n\r\nthe \".statelessClient(..)\" returns an instance of `StatelessClientConfig`, which has only a few\r\nof its own configurations, such as the `create()` method, which can be used to create a new\r\nstateless client.\r\nFor this example we ran both \r\nthe client and the server on the same host ( hence the “localhost\" setting ), \r\nbut in a real scanario the stateless client will typically be on a different server than its\r\nstatefull host. If you are aiming to create a stateless client and server on the same host, it's\r\nbetter not to do this, as the stateless client connects to the server via TCP/IP. It would be better to\r\nshare the maps via memory as this will give you better performance ( read more about\r\nthis at [Sharing Data Between Two or More Maps](https://github\r\n.com/OpenHFT/Chronicle-Map#sharing-data-between-two-or-more-maps).\r\n\r\nclick [here](https://github.com/OpenHFT/Chronicle-Map#sharing-data-between-two-or-more-maps ) \r\n\r\n### How to speed up the Chronicle Map Stateless Client \r\n\r\nWhen calling the stateless client, you will get better throughput if you invoke your requests from a\r\n number of threads, this is because by default when you make a method call to a `ChronicleMap`\r\n stateless client, your method call is wrapped into an event which is sent over TCP and processed\r\n  by the server. Your stateless client will block until an acknowledgement has been received from the server that\r\nthe event was processed.\r\n\r\n![Chronicle Map](http://openhft.net/wp-content/uploads/2014/09/State-Transition_2-thread_03.jpg)\r\n\r\nWhen you are calling methods that return a value like get() this\r\nblocking adds no additional overhead, because you have to wait for the return value anyway, \r\nIn some cases you could get better performance if you don't have to wait for the acknowledgement, This maybe the case when you are calling the `put()` method, but the problem with this method is it returns the old value even though you may not use it.\r\n\r\nFor in memory data-structures like a HashMap this isn’t a big problem. But in a distributed environment things are a bit more complicated. Blocking for an old value that you don’t require adds additional network overhead and additional processing overhead of serialising the old value on the server and deserialising it on the client\r\n\r\nSo if you don’t require the old value and don’t wish to block until your `put()` has been received by the server, then you may wish to consider using the following configuration :\r\n\r\n``` java\r\n.putReturnsNull(true)\r\n```\r\n  \r\nand also for the `remove()` method\r\n\r\n``` java\r\n.removeReturnsNull(true)\r\n```\r\n\r\n``` java\r\nstatelessClientMap = ChronicleMapBuilder.of(Integer.class, CharSequence.class)\r\n     .putReturnsNull(true)\r\n     .removeReturnsNull(true)\r\n     .statelessClient(new InetSocketAddress(\"localhost\", 8076))\r\n     .create();\r\n```\r\n\r\nFor the very best performance you should also set these properties on the server as well\r\n\r\n``` java\r\nChronicleMapBuilder.of(Integer.class, CharSequence.class)\r\n    .replication((byte) 2, TcpTransportAndNetworkConfig.of(8076))\r\n    .putReturnsNull(true)\r\n    .removeReturnsNull(true)\r\n    .create();            \r\n```\r\n\r\n##### Performance\r\n\r\nThe throughput and latency performance for different configurations.\r\n\r\nTested using a test called BGChronicleTest.\r\n\r\nOn one machine (i7 3.5 GHz) we have two persisted replicas run as\r\n\r\n-Dreplicas=2 eg.BGChronicleTest server\r\n\r\nOn another machine (Dual Xeon 8 core 2.6 GHz) connected via a pair of Solarflare SFN5121T 10 Gig-E with onload enabled.\r\n\r\n-Dreplicas=2 -Dclients={see below} -DmaxRate=30000 -DreadRatio=2 eg.BGChronicleTest client\r\n\r\n2 clients\r\nThroughput test\r\nmessages per seconds: 58,864\r\n\r\nLatency test at 30,000 msg/sec\r\n50% / 90% / 99% // 99.9% / 99.99% / worst latency was 33 / 80 / 111 // 120 / 148 / 3,921 us\r\n\r\n4 clients\r\nThroughput test\r\nmessages per seconds: 94,006\r\n\r\nLatency test at 30,000 msg/sec\r\n50% / 90% / 99% // 99.9% / 99.99% / worst latency was 32 / 94 / 106 // 131 / 177 / 2,153 us\r\n\r\n8 clients\r\nThroughput test\r\nmessages per seconds: 162,961\r\n\r\nLatency test at 30,000 msg/sec\r\n50% / 90% / 99% // 99.9% / 99.99% / worst latency was 35 / 94 / 117 // 140 / 167 / 1,685 us\r\n\r\n16 clients\r\nThroughput test\r\nmessages per seconds: 267,097\r\n\r\nLatency test at 30,000 msg/sec\r\n50% / 90% / 99% // 99.9% / 99.99% / worst latency was 38 / 97 / 122 // 149 / 174 / 2,771 us\r\n\r\n24 clients\r\nThroughput test\r\nmessages per seconds: 253,052\r\n\r\nLatency test at 30,000 msg/sec\r\n50% / 90% / 99% // 99.9% / 99.99% / worst latency was 40 / 99 / 121 // 151 / 243 / 3,669 us\r\n\r\n##### Close\r\n\r\nits always important to close `ChronicleMap`'s and `ChronicleSet` 's when you have finished with them\r\n\r\n``` java\r\nserverMap.close();\r\nstatelessMap.close();\r\n``` \r\n\r\n#  Known Issues\r\n\r\n##### Memory issue on Windows\r\n\r\nChronicleMap lets you assign a map larger than your available memory, If you were to create more\r\nentries than the available memory, ChronicleMap will page the segments that are accessed least to\r\ndisk, and load the recently used segments into available memory. This feature lets you work with\r\nextremely large maps, it works brilliantly on Linux but unfortunately, this paging feature is not\r\nsupported on Windows, if you use more memory than is physically available on windows you will\r\nexperience the following error :\r\n\r\n```java\r\nJava frames: (J=compiled Java code, j=interpreted, Vv=VM code)\r\nj sun.misc.Unsafe.compareAndSwapLong(Ljava/lang/Object;JJJ)Z+0\r\nj net.openhft.lang.io.NativeBytes.compareAndSwapLong(JJJ)Z+13\r\nj net.openhft.lang.io.AbstractBytes.tryLockNanos8a(JJ)Z+12\r\nj net.openhft.lang.io.AbstractBytes.tryLockNanosLong(JJ)Z+41\r\nj net.openhft.collections.AbstractVanillaSharedHashMap$Segment.lock()V+12\r\n```\r\n\r\n##### When Chronicle Map is Full\r\n\r\nIt will throw this exception :\r\n\r\n```java\r\nCaught: java.lang.IllegalStateException: VanillaShortShortMultiMap is full\r\njava.lang.IllegalStateException: VanillaShortShortMultiMap is full\r\n\tat net.openhft.collections.VanillaShortShortMultiMap.nextPos(VanillaShortShortMultiMap.java:226)\r\n\tat net.openhft.collections.AbstractVanillaSharedHashMap$Segment.put(VanillaSharedHashMap.java:834)\r\n\tat net.openhft.collections.AbstractVanillaSharedHashMap.put0(VanillaSharedHashMap.java:348)\r\n\tat net.openhft.collections.AbstractVanillaSharedHashMap.put(VanillaSharedHashMap.java:330)\r\n```\r\n\r\nChronicleMap doesn't resize automatically.  It is assumed you will make the virtual size of the map\r\nlarger than you need and it will handle this reasonably efficiently. With the default settings you\r\nwill run out of space between 1 and 2 million entries.\r\n\r\nYou should set the .entries(..) and .entrySize(..) to those you require.\r\n\r\n##### Don't forget to set the EntrySize\r\n\r\nIf you put() and entry that is much larger than the max entry size set via entrySize(), \r\nthe code will error. To see how to set the entry size the example below sets the entry size to 10, \r\nyou should pick a size that is the size in bytes of your entries : \r\n\r\n```java\r\nChronicleMap<Integer, String> map =\r\n             ChronicleMapBuilder.of(Integer.class, String.class)\r\n                     .entrySize(10).create();\r\n \r\n```\r\n\r\nThis example will throw an java.lang.IllegalArgumentException because the entrySize is too small.\r\n\r\n```java\r\n@Test\r\npublic void test() throws IOException, InterruptedException {\r\n    ChronicleMap<Integer, String> map =\r\n            ChronicleMapBuilder.of(Integer.class, String.class)\r\n                    .entrySize(10).create();\r\n\r\n    String value =   new String(new char[2000]);\r\n    map.put(1, value);\r\n\r\n    Assert.assertEquals(value, map.get(1));\r\n}\r\n\r\n```\r\n\r\nIf the entry size is dramatically too small ( like in the example below ), \r\nyou will get a *malloc_error_break* :\r\n\r\n```java\r\n@Test\r\npublic void test() throws IOException, InterruptedException {\r\n    ChronicleMap<Integer, String> map =\r\n            ChronicleMapBuilder.of(Integer.class, String.class)\r\n                    .entrySize(10).create();\r\n\r\n    String value =   new String(new char[20000000]);\r\n    map.put(1, value);\r\n\r\n    Assert.assertEquals(value, map.get(1));\r\n}\r\n```\r\n\r\n# Example : Simple Hello World\r\n\r\nThis simple chronicle map, works just like ConcurrentHashMap but stores its data off-heap. If you\r\nwant to use Chronicle Map to share data between java process you should look at the next exampl \r\n\r\n``` java \r\nMap<Integer, CharSequence> map = ChronicleMapBuilder.of(Integer.class,\r\n        CharSequence.class).create();\r\n\r\nmap.put(1, \"hello world\");\r\nSystem.out.println(map.get(1));\r\n\r\n``` \r\n\r\n# Example : Sharing the map on two ( or more ) processes on the same machine\r\n\r\nLets assume that we had two server, lets call them server1 and server2, if we wished to share a map\r\nbetween them, this is how we could set it up\r\n\r\n``` java \r\n\r\n// --- RUN ON ONE JAVA PROCESS ( BUT ON THE SAME SERVER )\r\n{\r\n    File file = new File(\"a-new-file-on-your-sever\");\t\r\n    Map<Integer, CharSequence> map1 = ChronicleMapBuilder.of(Integer.class, CharSequence.class)\r\n            .createPersistedTo(file); // this has to be the same file as used by map 2\r\n    map1.put(1, \"hello world\");\r\n}\r\n\r\n// --- RUN ON THE OTHER JAVA PROCESS ( BUT ON THE SAME SERVER )\r\n{\r\n    File file = new File(\"a-new-file-on-your-sever\");  // this has to be the same file as used by map 1\r\n    Map<Integer, CharSequence> map1 = ChronicleMapBuilder.of(Integer.class, CharSequence.class)\r\n            .createPersistedTo(file);\r\n\r\n    System.out.println(map1.get(1));\r\n}\r\n```\r\n\r\n# Example : Replicating data between process on different servers via TCP/IP\r\n\r\nLets assume that we had two server, lets call them server1 and server2, if we wished to share a map\r\nbetween them, this is how we could set it up\r\n\r\n``` java \r\nimport org.junit.Assert;\r\nimport org.junit.Test;\r\n\r\nimport java.io.IOException;\r\nimport java.net.InetSocketAddress;\r\nimport java.util.Map;\r\nimport java.util.concurrent.TimeUnit;\r\n\r\npublic class YourClass {\r\n\r\n    @Test\r\n    public void test() throws IOException, InterruptedException {\r\n\r\n        Map map1;\r\n        Map map2;\r\n\r\n//  ----------  SERVER1 1 ----------\r\n        {\r\n\r\n            // we connect the maps via a TCP/IP socket connection on port 8077\r\n\r\n            TcpTransportAndNetworkConfig tcpConfig = TcpTransportAndNetworkConfig\r\n                    .of(8076, new InetSocketAddress(\"localhost\", 8077))\r\n                    .heartBeatInterval(1L, TimeUnit.SECONDS);\r\n            ChronicleMapBuilder<Integer, CharSequence> map1Builder =\r\n                    ChronicleMapBuilder.of(Integer.class, CharSequence.class)\r\n                            .entries(20000L)\r\n                            .replication((byte) 1, tcpConfig);\r\n\r\n            map1 = map1Builder.create();\r\n        }\r\n//  ----------  SERVER2 on the same server as ----------\r\n\r\n        {\r\n            TcpTransportAndNetworkConfig tcpConfig =\r\n                    TcpTransportAndNetworkConfig.of(8077)\r\n                    .heartBeatInterval(1L, TimeUnit.SECONDS);\r\n            ChronicleMapBuilder<Integer, CharSequence> map2Builder =\r\n                    ChronicleMapBuilder.of(Integer.class, CharSequence.class)\r\n                            .entries(20000L)\r\n                            .replication((byte) 2, tcpConfig);\r\n            map2 = map2Builder.create();\r\n\r\n            // we will stores some data into one map here\r\n            map2.put(5, \"EXAMPLE\");\r\n        }\r\n\r\n//  ----------  CHECK ----------\r\n\r\n// we are now going to check that the two maps contain the same data\r\n\r\n// allow time for the recompilation to resolve\r\n        int t = 0;\r\n        for (; t < 5000; t++) {\r\n            if (map1.equals(map2))\r\n                break;\r\n            Thread.sleep(1);\r\n        }\r\n\r\n        Assert.assertEquals(map1, map2);\r\n        Assert.assertTrue(!map1.isEmpty());\r\n    }\r\n\r\n}\r\n```\r\n\r\n# Example : Replicating data between process on different servers using UDP\r\n\r\nThis example is the same as the one above, but it uses a slow throttled TCP/IP connection to fill in\r\nupdates that may have been missed when sent over UDP. Usually on a good network, for example a wired\r\nLAN, UDP won’t miss updates. But UDP does not support guaranteed delivery, we recommend also running\r\na TCP connection along side to ensure the data becomes eventually consistent.  Note : It is possible\r\nto use Chronicle without the TCP replication and just use UDP (  that’s if you like living dangerously ! )\r\n\r\n``` java\r\nimport org.junit.Assert;\r\nimport org.junit.Test;\r\n\r\nimport java.io.IOException;\r\nimport java.net.Inet4Address;\r\nimport java.net.InetSocketAddress;\r\nimport java.util.Map;\r\nimport java.util.concurrent.TimeUnit;\r\n\r\npublic class YourClass {\r\n\r\n    @Test\r\n    public void test() throws IOException, InterruptedException {\r\n\r\n        Map map1;\r\n        Map map2;\r\n\r\n        int udpPort = 1234;\r\n\r\n//  ----------  SERVER1 1 ----------\r\n        {\r\n\r\n            // we connect the maps via a TCP socket connection on port 8077\r\n\r\n            TcpTransportAndNetworkConfig tcpConfig = TcpTransportAndNetworkConfig\r\n                    .of(8076, new InetSocketAddress(\"localhost\", 8077))\r\n                    .heartBeatInterval(1L, TimeUnit.SECONDS)\r\n\r\n                            // a maximum of 1024 bits per millisecond\r\n                    .throttlingConfig(ThrottlingConfig.throttle(1024, TimeUnit.MILLISECONDS));\r\n\r\n            UdpTransportConfig udpConfig = UdpTransportConfig\r\n                    .of(Inet4Address.getByName(\"255.255.255.255\"), udpPort);\r\n\r\n            ChronicleMapBuilder<Integer, CharSequence> map1Builder =\r\n                    ChronicleMapBuilder.of(Integer.class, CharSequence.class)\r\n                            .entries(20000L)\r\n                            .replication(SingleChronicleHashReplication.builder()\r\n                                    .tcpTransportAndNetwork(tcpConfig)\r\n                                    .udpTransport(udpConfig)\r\n                                    .createWithId((byte) 1));\r\n\r\n            map1 = map1Builder.create();\r\n        }\r\n//  ----------  SERVER2 2 on the same server as ----------\r\n\r\n        {\r\n            TcpTransportAndNetworkConfig tcpConfig =\r\n                    TcpTransportAndNetworkConfig.of(8077)\r\n                    .heartBeatInterval(1L, TimeUnit.SECONDS)\r\n                    .throttlingConfig(ThrottlingConfig.throttle(1024, TimeUnit.MILLISECONDS));\r\n\r\n            UdpTransportConfig udpConfig = UdpTransportConfig\r\n                    .of(Inet4Address.getByName(\"255.255.255.255\"), udpPort);\r\n\r\n            ChronicleMapBuilder<Integer, CharSequence> map2Builder =\r\n                    ChronicleMapBuilder.of(Integer.class, CharSequence.class)\r\n                            .entries(20000L)\r\n                            .replication(SingleChronicleHashReplication.builder()\r\n                                    .tcpTransportAndNetwork(tcpConfig)\r\n                                    .udpTransport(udpConfig)\r\n                                    .createWithId((byte) 2));\r\n\r\n            map2 = map2Builder.create();\r\n\r\n            // we will stores some data into one map here\r\n            map2.put(5, \"EXAMPLE\");\r\n        }\r\n\r\n//  ----------  CHECK ----------\r\n\r\n// we are now going to check that the two maps contain the same data\r\n\r\n// allow time for the recompilation to resolve\r\n        int t = 0;\r\n        for (; t < 5000; t++) {\r\n            if (map1.equals(map2))\r\n                break;\r\n            Thread.sleep(1);\r\n        }\r\n\r\n        Assert.assertEquals(map1, map2);\r\n        Assert.assertTrue(!map1.isEmpty());\r\n    }\r\n}\r\n```\r\n\r\n# Example : Creating a Chronicle Set and adding data to it\r\n\r\nThis project also provides the Chronicle Set, `ChronicleSet` is built on Chronicle Map, so the builder\r\nconfiguration are almost identical to `ChronicleMap` ( see above ), this example shows how to create\r\na simple off heap set\r\n``` java \r\n        Set<Integer> set = ChronicleSetBuilder.of(Integer.class).create();\r\n        \r\n        set.add(1);\r\n        set.remove(1)\r\n```\r\nand just like map it support shared memory and TCP replication.         \r\n        \r\n        \r\n# Performance Topics\r\n\r\nThere are general principles we can give direction on - for specific advise we believe consulting\r\n to be the most productive solution.\r\n\r\nWe want the Map to be reasonably general purpose, so in broad terms we can say\r\n- the key and values have to be self contained, ideally trees, rather than graphs.\r\n- ideally values are similar lengths, however we support varying lengths.\r\n- ideally you want to use primitives and use object recycling for performance, though this is not a requirement.\r\n- ideally you have some idea as to the maximum number of entries, though it is not too important if\r\nthe maximum entries is above what you need.\r\n- if for example you are working with, market depth, this  can be supported via an array of nested \r\ntypes.\r\n- we support code generation of efficient custom serializes - See the examples where you provide \r\nan interface as the data type, the map will generate the implementation.\r\n\r\n### Improving the performance of Chronicle Maps Serialization\r\n\r\nAre you finding that when using Chronicle Map the serialisation is slow, but you have a large object graph,\r\nif so -\r\nhere are some steps that you can take to improve performance ( you may find that just one of these steps gives you the performance you require )\r\n\r\n* consider replacing java.io.Serializable with java.io.Externalizable ( java.io.Externalizable can be much\r\nfaster, but requires you do the the serialisation your self)\r\n* consider using net.openhft.lang.io.serialization.BytesMarshallable, working with this is like using java\r\n.io.Externalizable but in addition is supports some compressed types, which can give you slightly better performance.\r\n* try where possible to de-normalize your object graph, if you are able to limit your graph down to just a\r\nfew types you can then look at using our off heap interface proxy objects and have a map instance per type\r\nof object\r\n ( much like in a database where you would have a table per type ), you could then set up foreign key\r\n relationships between the maps, by using for example integers or longs. This works especially well as\r\n integers and longs have a good hash distribution.\r\n\r\n### Tuning Chronicle Map with Large Data\r\n\r\nGenerally speaking `ChronicleMap` is slower then ConcurrentHashMap for a small number of entries, but\r\nfor a large number of entries ConcurrentHashMap doesn't scale as well as Chronicle Map, especially\r\nwhen you start running low on heap. ConcurrentHashMap quickly becomes unusable whereas Chronicle Map\r\ncan still work when it is 20 times the size of a ConcurrentHashMap with an Out of Memory Error.\r\n  \r\nFor example with a heap of 3/4 of say 32 GB main memory, you might get say 100 million entries but\r\nwhen using most of the heap you might see 20-40 second gc pauses with `ChronicleMap` you could have\r\n1000 million entries and see < 100 ms pauses (depending on your disk subsystem and how fast you\r\nwrite your data)\r\n\r\nChronicle Map makes heavy use of the OS to perform the memory management and writing to disk. How it\r\nbehaves is very dependant on how you tune the kernel and what hardware you are using. You may get\r\nbad behaviour when the kernel forces a very large amount of data to disk after letting a lot of\r\nuncommited data build up. In the worst case scenario the OS will stop the process for tens of\r\nseconds at a time ( even up to 40 seconds) rather than let the program continue. However, to get\r\ninto that state you have to be loading a lot of data which exceeds main memory with very little rest\r\n(e.g. cpu processing). There are good use cases for bulk data loads, but you have to be careful how\r\nthis is done if you also want good worst case latency characteristics. (the throughput should be\r\nmuch the same)\r\n\r\nWhen you create a ChronicleMap, it has many segments. By default it has a minimum of 128, but one\r\nfor every 32 K entries. e.g. for 500M entries you can expect ~16K segments (being the next power of\r\n2). With so many segments, the chances of a perfect hash distribution is low and so the Chronicle\r\nMap allows for double what you asked for but is designed to do this with almost no extra main memory\r\n(only extra virtual memory). This means when you ask for 500M * 256 bytes entries you actually get 1\r\nBN possible entries (assuming a perfect hash distribution between segments) There is a small\r\noverhead per entry of 16 - 24 bytes adding another 20 GB.\r\n\r\nSo while the virtual memory is 270 GB, it is expected that for 500 M entries you will be trying to\r\nuse no more than 20 GB (overhead/hash tables) + ~120 GB (entries)\r\n\r\nWhen `ChronicleMap` has exhausted all the memory on your server, its not going to be so fast, for a\r\nrandom access pattern you are entirely dependant on how fast your underlying disk is. If your home\r\ndirectory is an HDD and its performance is around 125 IOPS (I/Os per second). Each lookup takes two\r\nmemory accesses so you might get around 65 lookups per second. For 100-200K operations you can\r\nexpect around 1600 seconds or 25-50 minutes. If you use an SSD, it can get around 230 K IOPS, or\r\nabout 115 K `ChronicleMap` lookups per second.\r\n\r\n### Lock contention\r\n\r\nIf you see the following warning :\r\n\r\n``` java \r\nWARNING:net.openhft.lang.io.AbstractBytes tryLockNanosLong0\r\nWARNING: Thread-2, to obtain a lock took 0.129 seconds\r\n``` \r\n \r\nIt's likely you have lock contention, this can be due to : \r\n\r\n- a low number of segments and\r\n- the machine was heavily over utilised, possibly with the working data set larger than main memory.\r\n- you have a large number of threads, greater than the number of cores you have, doing nothing but hit one map.\r\n\r\nIt’s not possible to fully disable locking,  locking is done a a segment basis.\r\nSo, If you set a large number of actual segments, this will reduce your lock contention. \r\n\r\nSee the example below to see how to set the number of segments :\r\n\r\n``` java \r\nChronicleMap<Long, String> map = ChronicleMapBuilder.of(Long.class, String.class)\r\n   .entries(100)\r\n   .actualSegments(100)    // set your number of segments here\r\n   .create();\r\n```  \r\n \r\nReducing lock contention will make this warning message go away, but this message maybe more of a symptom \r\nof a general problem with what the system is doing, so you may experience a delay anyway.\r\n\r\n### Better to use small keys\r\n\r\nIf you put() a small number of large entries into ChronicleMap, you are unlikely to see any\r\nperformance gains over a standard map, So we recommend you use a standard ConcurrentHashMap, unless\r\nyou need ChronicleMaps other features.\r\n\r\nChronicle Map gives better performance for smaller keys and values due to the low overhead per\r\nentry. It can use 1/5th the memory of ConcurrentHashMap. When you have larger entries, the overhead\r\nper entry doesn't matter so much and the relative waste per entry starts to matter. For Example,\r\nChronicleMap assumes every entry is the same size and if you have 10kB-20kB entries the 10K entries\r\ncan be using 20 kB of virtual memory or at least 12 KB of actual memory (since virtual memory turns\r\ninto physical memory in multiples of a page)\r\n\r\nAs the `ChronicleMap` gets larger the most important factor is the use of CPU cache rather than main\r\nmemory, performance is constrained by the number of cache lines you have to touch to update/read an\r\nentry. For large entries this is much the same as ConcurrentHashMap.  In this case, `ChronicleMap` is\r\nnot worse than ConcurrentHashMap but not much better.\r\n\r\nFor large key/values it is not total memory use but other factors which matter such as;\r\n- how compact each entry is. Less memory used makes better use of the L3 cache and memory bus which\r\n  is often a bottleneck in highly concurrent applications. \r\n- reduce the impact on GCs. The time to perform  GC and its impact is linear. Moving the bulk of\r\n  your data off heap can dramatically improve throughput not to mention worst case latency.\r\n- Large data structures take a long time to reload and having a persisted store significantly\r\n  reduces restart times.\r\n- data can be shared between processes. This gives you more design options to share between JVMS and\r\n  support short lived tasks without having to use TCP.\r\n- data can be replicated across machines.\r\n\r\n### ConcurrentHashMap v ChronicleMap\r\nConcurrentHashMap ( CHM ) outperforms `ChronicleMap` ( CM ) on throughput.  If you don't need\r\nthe extra features ChronicleMap gives you, it is not worth the extra complexity it brings.\r\ni.e. don't use it just because you think it is cool. The test can be found in\r\n[ChronicleMapTest](https://github.com/OpenHFT/Chronicle-Map/blob/master/src/test/java/net/openhft/chronicle/map/ChronicleMapTest.java)\r\nunder testAcquirePerf() and testCHMAcquirePerf()\r\n\r\nChronicleMap outperforms ConcurrentHashMap on memory consumption, and worst case latencies.\r\nIt can be used to reduce or eliminate GCs.\r\n\r\n#### Performance Test for many small key-values\r\nThe following performance test consists of string keys of the form \"u:0123456789\" and an int\r\ncounter.  The update increments the counter once in each thread, creating an new entry if required.\r\n\r\n| Number of entries | Chronicle* Throughput  |  Chronicle RSS  | HashMap* Throughput | HashMap Worst GC pause | HashMap RSS |\r\n|------------------:|---------------:|---------:|---------------:|-------------------:|--------:|\r\n|        10 million |      30 Mupd/s |     ½ GB |     155 Mupd/s |           2.5 secs |    9 GB |\r\n|        50 million |      31 Mupd/s |    3⅓ GB |     120 Mupd/s |           6.9 secs |   28 GB |\r\n|       250 million |      30 Mupd/s |    14 GB |     114 Mupd/s |          17.3 secs |   76 GB |\r\n|      1000 million |      24 Mupd/s |    57 GB |           OOME |            43 secs |      NA |\r\n|      2500 million |      23 Mupd/s |   126 GB |   Did not test |                 NA |      NA |\r\n\r\n_*HashMap refers to ConcurrentHashMap, Chronicle refers to Chronicle Map_\r\n\r\n\r\nKey :\r\nRSS - Resident memory size.  How much main memory was used.\r\nMupd/s - Million write operations per second. i.e. put(key, value);\r\n\r\n\r\nNotes:\r\n* `ChronicleMap` was tested with a 32 MB heap, CHM was test with a 100 GB heap.\r\n* The `ChronicleMap` test had a small minor GC on startup of 0.5 ms, but not during the test.\r\n  This is being investigated.\r\n* `ChronicleMap` was tested \"writing\" to a tmpfs file system.\r\n\r\n#### How does it perform when persisted?\r\n\r\nChronicle Map also supports persistence. In this regard there is no similar class in the JDK.\r\n\r\n| Number of entries | Chronicle Throughput  |  Chronicle RSS |\r\n|------------------:|---------------:|---------:|\r\n|        10 million |      28 Mupd/s |     ½ GB |\r\n|        50 million |      28 Mupd/s |     9 GB |\r\n|       250 million |      26 Mupd/s |    24 GB |\r\n|      1000 million |     1.3 Mupd/s |    85 GB |\r\n\r\nNotes:\r\n* Persistence was performed at a PCI-SSD which supports up to 230K IOPS and 900 MB/s write speed.\r\n  This test didn't test the card to it's limit until the last test.\r\n* The kernel tuning parameters for write back are important here.\r\n  This explains the suddern drop off and this is being investigated.\r\n\r\nThe sysctl parameters used were approximately 10x the defaults to allow as many operations\r\nto be performed in memory as possible.\r\n\r\n    vm.dirty_background_ratio = 50\r\n    vm.dirty_expire_centisecs = 30000\r\n    vm.dirty_ratio = 90\r\n    vm.dirty_writeback_centisecs = 5000\r\n\r\n# Questions and Answers\r\n\r\n#### Question\r\nI'm searching for a Map implementation that is backed by either direct off-heap or a memory \r\nmapped file. I want to use it as write-once, read-n-times kind of cache.\r\n#### Answer\r\nThe latest version of ChronicleMap has been optimised for this use case. Ie for heavy read to write ratios. \r\nNote: heavy writers will see about the same performance.\r\n\r\n---\r\n\r\n#### Question\r\nThe backing of to off-heap is only needed to prevent OOM-situations. Otherwise the process  can use a big part of the available memory for the map.\r\n\r\n#### Answer\r\nChronicle Map can be larger than main memory. In fact you might want to reduce the heap size to give chronicle map more memory in extreme cases. Ie because the map can't utilise on heap memory much.\r\n\r\n---\r\n\r\n#### Question\r\nI stumbled over chronicle-map and it looks like it could do most of those things. Correct me if i'm wrong but\r\n``` java\r\n// would create the off-heap version \r\nChronicleMapBuilder.of(A.class, B.class).create();\r\n```\r\n#### Answer\r\nYes.\r\n\r\n---\r\n\r\n#### Question\r\n``` java\r\n// the memory-mapped file version ?\r\n ChronicleMapBuilder.of(A.class, B.class).file(mapFile).create();\r\n```\r\n#### Answer\r\nYes.\r\n\r\n---\r\n\r\n#### Question\r\nBut both versions write the complete map at every single moment to off-heap, right ?\r\n\r\n#### Answer\r\nEvery key/value is off heap. There is no cache of objects on heap.\r\n\r\n---\r\n\r\n#### Question\r\nIs there something in chronicle map which can have part of the map-content still in on-heap memory.\r\n\r\n#### Answer\r\nThere are data structures which are on heap to manahe the off heap data. This is typically less than 100 kB.\r\n\r\n---\r\n\r\n#### Question\r\nEither serialized or deserialized. Performance wise do you think that would even matter ?\r\n#### Answer\r\nIt matter a lot but you can get the performance close to on heap objects without the long GC pause times. Ie only the bit on the heap makes any difference.\r\n\r\n---\r\n\r\n#### Question\r\nLast but not least... do you think chronicle is suitable for this simple purpose ?\r\n\r\n#### Answer\r\nActually this is all it does really. We try to make it fast by keeping it simple. \r\nEg it is not a cache and doesn't have expiries etc. There is no background thread managing it.\r\nIn fact it is so simple that many methods are completely in lined. \r\nIe if you look at a crash dump the chronicle methods have been completely in lined and don't appear in the stack trace any more. (The still appear in java stack traces)\r\n\r\n---\r\n\r\n#### Question\r\nOr is there a lot of overhead for other functionality which i do not intend to use (yet) !?\r\n\r\n#### Answer\r\nYou can share the data between processes on the same machine but this doesn't add overhead.\r\nYou can add replication between machines. But this uses extra classes which are not used in the simple case ie it is as if they were not there, only an option.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}